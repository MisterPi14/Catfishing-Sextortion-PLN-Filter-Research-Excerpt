{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Codigo que soporta los enfoques ZERO SHOT y FEW SHOT APROACH SINTETIC/REAL \n",
    "# (Necesita cambiar la ruta en EXAMPLES_DIR) + DICTIONARY 15/06/2025\n",
    "# Carga ejemplos y diccionario desde JSON en scam-examples/sintetic\n",
    "# Incluye el diccionario de regionalismos en el prompt.\n",
    "# filepath: c:\\Users\\Diego\\Desktop\\PLN fIlter\\ModelsClassificationTests.ipynb\n",
    "\n",
    "import subprocess\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import statistics\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from ollama import Client\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# ----------------------------\n",
    "#OUTPUT_DIR = 'LMMs-Classification-Test-Results/Few-Shot-Sintetic-Dictionary-Definitions-Provided' # Nueva carpeta de salida\n",
    "OUTPUT_DIR = 'LMMs-Classification-Test-Results/Few-Shot-Sintetic-Aproach' # Nueva carpeta de salida\n",
    "\n",
    "PROMPT_VERSION = 'v1.3' # Versi√≥n actualizada\n",
    "APAGADO = False # Mantener la configuraci√≥n de apagado\n",
    "DEFINITIONS = False\n",
    "DICTIONARY = False\n",
    "EXAMPLES = True\n",
    "EXAMPLES_DIR = 'scam-examples/sintetic' # Directorio para los ejemplos y diccionarios JSON\n",
    "SELECTED_MODELS_ONLY = True\n",
    "MODELS_SELECTION = [\"mxbai-embed-large:335m\"]\n",
    "# ASUMPCI√ìN: La variable 'dataset' est√° definida en una celda anterior y es accesible globalmente.\n",
    "# ASUMPCI√ìN: El cliente 'client' de Ollama est√° inicializado.\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# FUNCIONES AUXILIARES\n",
    "# ----------------------------\n",
    "\n",
    "def load_scam_examples_and_dictionary():\n",
    "    examples = \"\"\n",
    "    dictionary = \"\"\n",
    "\n",
    "    if EXAMPLES:\n",
    "        examples += \"ADDITIONAL EXAMPLES FOR CONTEXT (Pay close attention to these examples to understand the nuances of each category): \"\n",
    "    if DICTIONARY:\n",
    "        dictionary += \"DICTIONARY OF MEXICAN REGIONALISMS (These terms are common in Mexico City colloquial language and may appear in messages): \"\n",
    "    \n",
    "    # Cargar ejemplos adicionales\n",
    "    \n",
    "    if os.path.exists(EXAMPLES_DIR):\n",
    "        for filename in os.listdir(EXAMPLES_DIR):\n",
    "            if filename.endswith('.json'):\n",
    "                with open(os.path.join(EXAMPLES_DIR, filename), 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    if EXAMPLES is True:\n",
    "                        examples += f\"{os.path.splitext(filename)[0]}: \" #para solo extraer el nombre de los archivos sin extencion\n",
    "                        for i, example in enumerate(data.get('examples', []), 1):\n",
    "                            examples += f\"message {i}: {example['text']}, \"\n",
    "                    if DICTIONARY is True:\n",
    "                        for entry in data.get('dictionary', []):\n",
    "                            dictionary += f\"{entry['word']}: \\\"{entry['meaning']},\\\" \"\n",
    "        return examples, dictionary\n",
    "    else:\n",
    "        print(f\"Advertencia: El directorio de ejemplos '{EXAMPLES_DIR}' no fue encontrado.\")\n",
    "        return \"\", \"\"\n",
    "\n",
    "def get_training_dataset_size():\n",
    "    \"\"\"Cuenta el n√∫mero total de samples en todos los archivos JSON del directorio de ejemplos, desglosado por categor√≠a.\"\"\"\n",
    "    total_samples = 0\n",
    "    category_counts = {'catfishing': 0, 'sextortion': 0, 'harmless': 0}\n",
    "    \n",
    "    if os.path.exists(EXAMPLES_DIR):\n",
    "        for filename in os.listdir(EXAMPLES_DIR):\n",
    "            if filename.endswith('.json'):\n",
    "                try:\n",
    "                    with open(os.path.join(EXAMPLES_DIR, filename), 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "                        examples = data.get('examples', [])\n",
    "                        total_samples += len(examples)\n",
    "                        \n",
    "                        # Contar por categor√≠a\n",
    "                        for example in examples:\n",
    "                            category = example.get('category', '').lower()\n",
    "                            if category in category_counts:\n",
    "                                category_counts[category] += 1\n",
    "                                \n",
    "                except (json.JSONDecodeError, KeyError) as e:\n",
    "                    print(f\"Error contando samples en {filename}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    if total_samples > 0:\n",
    "        breakdown = f\"catfishing: {category_counts['catfishing']}, sextortion: {category_counts['sextortion']}, harmless: {category_counts['harmless']}\"\n",
    "        return f\"{total_samples} samples ({breakdown})\"\n",
    "    else:\n",
    "        return \"0 samples\"\n",
    "\n",
    "    \n",
    "def get_available_models():\n",
    "\n",
    "    # Siempre cargar modelos evaluados\n",
    "    evaluated_models = set()\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        for filename in os.listdir(OUTPUT_DIR):\n",
    "            if filename.endswith('.json'):\n",
    "                try:\n",
    "                    with open(os.path.join(OUTPUT_DIR, filename), 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "                        model_name = data.get('metadata', {}).get('model_name')\n",
    "                        if model_name:\n",
    "                            evaluated_models.add(model_name)\n",
    "                except (json.JSONDecodeError, KeyError) as e:\n",
    "                    print(f\"Error leyendo el archivo {filename}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    # Obtener informaci√≥n de modelos instalados usando ollama list (una sola vez)\n",
    "    installed_models = dict()\n",
    "    try:\n",
    "        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, check=True)\n",
    "        lines = result.stdout.strip().split('\\n')[1:]\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                model_name = line.split()[0]\n",
    "                model_size = line.split()[2] + \" \" + line.split()[3]\n",
    "                installed_models[model_name] = model_size\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error ejecutando 'ollama list': {e}\")\n",
    "        return []\n",
    "    except FileNotFoundError:\n",
    "        print(\"Comando 'ollama' no encontrado. Aseg√∫rate de que Ollama est√© instalado y en el PATH.\")\n",
    "        return []\n",
    "\n",
    "    if not SELECTED_MODELS_ONLY:\n",
    "        # Excluir modelos ya evaluados\n",
    "        models_to_evaluate = {model: size for model, size in installed_models.items() if model not in evaluated_models}\n",
    "    else:\n",
    "        models_to_evaluate = dict()\n",
    "        # trabaja MODELS_SELECTION como lista o como string unitario segun sea el caso\n",
    "        model_list = MODELS_SELECTION if isinstance(MODELS_SELECTION, list) else [MODELS_SELECTION]\n",
    "        for m in model_list:\n",
    "            if m not in evaluated_models and m in installed_models:\n",
    "                models_to_evaluate[m] = installed_models[m]\n",
    "            elif m not in installed_models:\n",
    "                print(f\"Advertencia: El modelo '{m}' no est√° instalado en Ollama, no se puede proseguir\")\n",
    "                sys.exit(1)\n",
    "\n",
    "    if evaluated_models:\n",
    "        print(f\"\\nü§ñ {len(list(evaluated_models))} modelos ya evaluados y excluidos:\\n\")\n",
    "        print(*sorted(list(evaluated_models)), sep=\"\\n\")\n",
    "    else:\n",
    "        print(\"Ningun modelo ha sido evaluado aun\")\n",
    "    \n",
    "    return models_to_evaluate\n",
    "\n",
    "def get_metadata(model_name, model_size, current_dataset, dictionary_text_provided):\n",
    "    \"\"\"Devuelve un diccionario con metadatos para incluir en todos los informes.\"\"\"\n",
    "    try:\n",
    "        code_version = subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).strip().decode('utf-8')\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        code_version = 'N/A'\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model_size': model_size,\n",
    "        'evaluation_date': datetime.utcnow().isoformat() + 'Z',\n",
    "        'prompt_version': PROMPT_VERSION,\n",
    "        'code_version (short-commit-hash)': code_version,\n",
    "        'training_dataset_size': get_training_dataset_size() if EXAMPLES else \"none examples given\",\n",
    "        'examples_directory': f\"{EXAMPLES_DIR if EXAMPLES else \"none examples given\"}\",\n",
    "        'test_dataset_size': str(len(current_dataset)) + ' samples (10 off each category)',\n",
    "        'dictionary_provided': bool(dictionary_text_provided),           \n",
    "        'definitions_provided': bool(DEFINITIONS)\n",
    "    }\n",
    "\n",
    "def ensure_output_dir():\n",
    "    \"\"\"Crea la carpeta de resultados si no existe.\"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def get_model_prediction(model_name, text, additional_examples_text=\"\", dictionary_text=\"\"):\n",
    "    \"\"\"Env√≠a prompt al modelo y extrae la etiqueta predicha.\"\"\"\n",
    "    prompt = (\n",
    "        \"ROLE: You are a cybersecurity expert specialized in detecting digital fraud and malicious online behavior. \"\n",
    "        \n",
    "        \"CONTEXT: You are analyzing text messages written in Mexico City colloquial language to identify fraudulent behavior patterns in digital communications. \"\n",
    "\n",
    "        f\"{dictionary_text}\" # INSERCI√ìN DEL DICCIONARIO DE REGIONALISMOS\n",
    "\n",
    "        f\"{\"DEFINITIONS OF THE SCAMS TO IDENTIFY \"\n",
    "        \"1.- catfishing: Creation of a fake online identity to emotionally deceive the victim, typically requesting money, personal information, or creating emotional dependency. \"\n",
    "        \"key differences: focused on establishing a romantic connection to manipulate the victim later, The tone of the characteristic speech is closer to deception, manipulation through emotional ties, persuasion and the lack of naturalness with which one tries to reach the link, usually ask for money for false emergencies or other contexts. \"\n",
    "        \"2.-sextortion: Sexual blackmail where the attacker threatens to publish intimate content of the victim unless they comply with demands (usually money). \"\n",
    "        \"key differences: it involves a video, photo or any other media, The tone of the characteristic speech is closer to blackmail, coercion, coercive or directly threatening action. \"\n",
    "        \"3.-harmless: Normal and legitimate communication without fraudulent or malicious intentions. \"\n",
    "        \"key differences: it does not represent any threat, casual conversations, real plans, cordial exchanges, normal romantic relationships conversations. \" if DEFINITIONS else ''}\"\n",
    "        \n",
    "        f\"{additional_examples_text}\" # INSERCI√ìN DE EJEMPLOS ADICIONALES\n",
    "        \n",
    "        \"APPROACH: Based on the previous, classify the following message into exactly one of these three categories: catfishing, sextortion, or harmless. \"\n",
    "        \n",
    "        f\"{\"The examples contain a real conversation that includes one of the 3 types of scams being considered. Identify the Attacker: and Victim: labels as they will indicate the participation of each actor. \" if EXAMPLES_DIR == 'scam-examples/real' and EXAMPLES else \"\"}\"\n",
    "\n",
    "        \"LIMITS: \"\n",
    "        \"- Respond ONLY with the corresponding label: catfishing, sextortion, or harmless. \"\n",
    "        \"- DO NOT include explanations, justifications, or any additional text. \"\n",
    "        \"- DO NOT use quotes, periods, or special characters in your response. Just the label. \"\n",
    "        \n",
    "        f\"Message to analyze: \\\"{text}\\\" \"\n",
    "    )\n",
    "    \n",
    "    # Petici√≥n directa sin streaming\n",
    "    response = client.chat(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}], stream=False)\n",
    "    response = response['message']['content'].strip().lower()\n",
    "    response_without_thought = re.sub(r'.*\\n+','',str(response), flags=re.DOTALL)#filtrado de la respuesta para eliminar la parte del pensamiento del texto\n",
    "    # Filtrar la etiqueta v√°lida\n",
    "    labels = ['catfishing', 'sextortion', 'harmless']\n",
    "    found_labels = [label for label in labels if label in response_without_thought]\n",
    "    #descartar si no se encontro una etiqueta valida o se encontraron mas de una\n",
    "    if len(found_labels) == 1:\n",
    "        prediction = found_labels[0]\n",
    "    else:\n",
    "        prediction = 'unclassified'\n",
    "    return response_without_thought, prediction\n",
    "\n",
    "def evaluate_model(model_name, dataset_to_use, additional_examples_text=\"\", dictionary_text=\"\"):\n",
    "    \"\"\"Ejecuta todas las predicciones y calcula m√©tricas para un modelo espec√≠fico.\"\"\"\n",
    "    y_true, y_pred_raw, y_pred_leaked = [], [], []\n",
    "    prediction_times = []\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    print(f\"Evaluando {len(dataset_to_use)} muestras con el modelo {model_name}...\")\n",
    "\n",
    "    print(\"Progreso: 0%\", end=\"\")\n",
    "\n",
    "    for i, sample in enumerate(dataset_to_use, 1):        \n",
    "        individual_pred_start_time = time.time()\n",
    "        rawPrediction, leakedPrediction = get_model_prediction(model_name, sample['text'], additional_examples_text, dictionary_text)\n",
    "        individual_pred_end_time = time.time()\n",
    "        \n",
    "        y_true.append(sample['label'])\n",
    "        y_pred_raw.append(rawPrediction)\n",
    "        y_pred_leaked.append(leakedPrediction)\n",
    "        prediction_times.append(individual_pred_end_time - individual_pred_start_time)\n",
    "\n",
    "        percentage = (i / len(dataset_to_use)) * 100\n",
    "        print(f\"\\rProgreso: {percentage:.1f}% ({i}/{len(dataset_to_use)} muestras)\", end=\"\", flush=True)\n",
    "    \n",
    "    total_end_time = time.time()\n",
    "    total_evaluation_time = total_end_time - total_start_time\n",
    "    \n",
    "    print(f\"\\nCompletado: {len(dataset_to_use)} muestras procesadas para {model_name}.\")\n",
    "    \n",
    "    possible_labels = sorted(list(set(y_true)))#se usa set para que no existan etiquetas repetidos\n",
    "    report_dict = classification_report(y_true, y_pred_leaked, output_dict=True, zero_division=0, labels=possible_labels)\n",
    "    report_dict['accuracy'] = accuracy_score(y_true, y_pred_leaked)\n",
    "    timing_metrics = {\n",
    "        'total_evaluation_time_seconds': total_evaluation_time,\n",
    "        'average_time_per_prediction_seconds': statistics.mean(prediction_times) if prediction_times else 0,\n",
    "        'min_prediction_time_seconds': min(prediction_times) if prediction_times else 0,\n",
    "        'max_prediction_time_seconds': max(prediction_times) if prediction_times else 0,\n",
    "        'samples_per_second': len(dataset_to_use) / total_evaluation_time if total_evaluation_time > 0 else 0,\n",
    "        'total_evaluation_time_minutes': total_evaluation_time / 60\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'labels' : {'etiquetas_reales':y_true, 'llm-output-predictions':y_pred_raw, 'leaked-prediction': y_pred_leaked},\n",
    "        'classification_report': report_dict,\n",
    "        'timing_metrics': timing_metrics\n",
    "    }\n",
    "\n",
    "def export_to_json(report_data, metadata_dict, filename_prefix):\n",
    "    \"\"\"Guarda el reporte con metadatos en un archivo JSON.\"\"\"\n",
    "    payload = {\n",
    "        'metadata': metadata_dict,\n",
    "        'results': report_data\n",
    "    }\n",
    "    safe_model_name = re.sub(r'[^\\w\\-_.]', '_', metadata_dict['model_name'])\n",
    "    date_str = metadata_dict['evaluation_date'].split('T')[0]\n",
    "    filename = f\"{safe_model_name}_eval_{date_str}.json\"\n",
    "    \n",
    "    path = os.path.join(OUTPUT_DIR, filename)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(payload, f, indent=4, ensure_ascii=False)\n",
    "    print(f'‚úÖ JSON guardado en {path}')\n",
    "\n",
    "def evaluate_all_models(current_dataset):\n",
    "    \"\"\"Eval√∫a todos los modelos disponibles en Ollama usando el dataset, ejemplos y diccionario.\"\"\"\n",
    "    ensure_output_dir()\n",
    "    \n",
    "    print(f\"Cargando ejemplos y diccionario desde: {EXAMPLES_DIR}\")\n",
    "    loaded_examples_text, loaded_dictionary_text = load_scam_examples_and_dictionary()\n",
    "    \n",
    "    if loaded_examples_text:\n",
    "        example_count = loaded_examples_text.count(\"<\")\n",
    "        print(f\"Se cargaron {example_count} ejemplos.\")\n",
    "    # No 'else' needed due to print within load_scam_examples_and_dictionary\n",
    "\n",
    "    if loaded_dictionary_text:\n",
    "        dict_entry_count = loaded_dictionary_text.count(\"\\n- \")\n",
    "        print(f\"Se cargaron {dict_entry_count} entradas de diccionario.\")\n",
    "    # No 'else' needed\n",
    "\n",
    "    models_to_evaluate = get_available_models()\n",
    "    \n",
    "    if not models_to_evaluate:\n",
    "        print(\"No se encontraron modelos nuevos para evaluar.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nü§ñ {len(list(models_to_evaluate))} modelos encontrados para evaluar:\\n\")\n",
    "    print(*list(models_to_evaluate), sep=\"\\n\") \n",
    "    print(f\"üìä Dataset con {len(current_dataset)} muestras.\")\n",
    "    print(f\"üìÅ Resultados se guardar√°n en: {OUTPUT_DIR}/\")\n",
    "    \n",
    "    for i, model_name_to_eval in enumerate(models_to_evaluate, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EVALUANDO MODELO {i}/{len(models_to_evaluate)}: {model_name_to_eval}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            current_metadata = get_metadata(model_name_to_eval, models_to_evaluate[model_name_to_eval], current_dataset, bool(loaded_dictionary_text))\n",
    "            \n",
    "            evaluation_report = evaluate_model(model_name_to_eval, current_dataset, loaded_examples_text, loaded_dictionary_text)\n",
    "            \n",
    "            export_to_json(evaluation_report, current_metadata, model_name_to_eval)\n",
    "            \n",
    "            print(f\"‚úÖ Evaluaci√≥n completada para {model_name_to_eval}\")\n",
    "            if 'classification_report' in evaluation_report and 'accuracy' in evaluation_report['classification_report']:\n",
    "                print(f\"   Accuracy: {evaluation_report['classification_report']['accuracy']:.4f}\")\n",
    "            if 'timing_metrics' in evaluation_report:\n",
    "                print(f\"   Tiempo total de evaluaci√≥n: {evaluation_report['timing_metrics']['total_evaluation_time_seconds']:.2f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error evaluando modelo {model_name_to_eval}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "# Inicializar cliente Ollama (asumiendo que ya est√° hecho o se har√° en otra celda)\n",
    "try:\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    client.list()\n",
    "    print(\"üîå Conexi√≥n con Ollama establecida exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå No se pudo conectar con Ollama en http://localhost:11434. Aseg√∫rate que est√© corriendo.\")\n",
    "    print(f\"Error: {e}\") \n",
    "    sys.exit(1)\n",
    "\n",
    "# EJECUTAR EVALUACI√ìN: Aseg√∫rate que la variable 'dataset' est√© definida y 'client' inicializado.\n",
    "if 'testDataset' in globals() and isinstance(testDataset, list) and len(testDataset) > 0 and 'client' in globals():\n",
    "    print(\"\\nüöÄ Iniciando evaluaci√≥n autom√°tica de todos los modelos...\")\n",
    "    evaluate_all_models(testDataset) \n",
    "    print(\"\\nüéâ Evaluaci√≥n de todos los modelos completada.\")\n",
    "    \n",
    "\n",
    "    if APAGADO is True:\n",
    "        print(\"\\nüí§ Programando apagado del equipo en 60 segundos...\")\n",
    "        try:\n",
    "            subprocess.run(['shutdown', '/s', '/t', '60'], check=True)\n",
    "            print(\"‚úÖ Apagado programado exitosamente.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error programando apagado: {e}\")\n",
    "else:\n",
    "    if 'testDataset' not in globals() or not isinstance(testDataset, list) or not len(testDataset) > 0:\n",
    "        print(\"‚ö†Ô∏è La variable 'testDataset' no est√° definida o est√° vac√≠a. Por favor, define el testDataset en una celda anterior.\")\n",
    "    if 'client' not in globals():\n",
    "        print(\"‚ö†Ô∏è La variable 'client' (Ollama client) no est√° inicializada. Por favor, inicial√≠zala.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
